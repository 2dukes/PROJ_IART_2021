{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.infante.space/wp-content/uploads/2018/02/Logo-FEUP.png\" width=\"35%\"/>\n",
    "    <h1 style=\"font-size: 2.5em\">Credit Risk Analysis</h1>\n",
    "    <h2 style=\"font-size: 2em\">Artificial Intelligence 2020/21 - Supervised Learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [License](#License)\n",
    "\n",
    "3. [Required libraries](#Required-libraries)\n",
    "\n",
    "4. [The problem domain](#The-problem-domain)\n",
    "\n",
    "5. [Step 1: Answering the question](#Step-1:-Answering-the-question)\n",
    "\n",
    "6. [Step 2: Checking the data](#Step-2:-Checking-the-data)\n",
    "\n",
    "7. [Step 3: Tidying the data](#Step-3:-Tidying-the-data)\n",
    "\n",
    "    - [Bonus: Testing our data](#Bonus:-Testing-our-data)\n",
    "\n",
    "8. [Step 4: Exploratory analysis](#Step-4:-Exploratory-analysis)\n",
    "\n",
    "9. [Step 5: Classification](#Step-5:-Classification)\n",
    "\n",
    "    - [Cross-validation](#Cross-validation)\n",
    "\n",
    "    - [Parameter tuning](#Parameter-tuning)\n",
    "\n",
    "10. [Step 6: Reproducibility](#Step-6:-Reproducibility)\n",
    "\n",
    "11. [Conclusions](#Conclusions)\n",
    "\n",
    "12. [Further reading](#Further-reading)\n",
    "\n",
    "13. [Acknowledgements](#Acknowledgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project we look forward to use Machine Learning, more specifically, Supervised Learning, to predict the risk on loan repayment. For this, we use a provided dataset with approximatelly 800k entries of previously issued loans and aim to train this dataset using various machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Please see the repository README file for the licenses and usage terms for the instructional material and code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "- **numpy** - provides support for large multidimensional arrays and matrices along with a collection of high-level mathematical functions to execute these functions swiftly.\n",
    "- **pandas** - enables the provision of easy data structure and quicker data analysis for Python. For operations like data analysis and modelling, Pandas makes it possible to carry these out without needing to switch to more domain-specific language.\n",
    "- **scikit-learn** - can be effectively used for a variety of applications which include classification, regression, clustering, model selection, naive Bayesâ€™, grade boosting, K-means, and preprocessing.\n",
    "- **matplotlib** - widely used for publication of quality figures in a variety of hard copy formats and interactive environments across platforms. Used to design charts, graphs, pie charts, scatterplots, histograms, error charts, etc.\n",
    "- **seaborn** - visualisation of statistical models like heat maps.\n",
    "- **watermark** - printing date and time stamps, version numbers, and hardware information.\n",
    "- **contextlib**\n",
    "- **time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifie\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Normalization\n",
    "\n",
    "After a thorough analysis of the data available, it was decided to drop a considerable amount of columns from the dataset. It includes columns containing either data that was not relevant to the problem or columns with too many missing values to be used reliably.\n",
    "For example: identifiers, titles and descriptions; zip codes.\n",
    "We also created new columns using old ones with, for example, date differences and differences of money amounts, replaced 'grade' column with the 'sub_grade' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (17,45,53) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./resources/data.csv', index_col=0)\n",
    "\n",
    "# Remove id column and use a default index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.drop(columns = [\n",
    "        'member_id',\n",
    "        'grade',\n",
    "        'emp_title',\n",
    "        'pymnt_plan',\n",
    "        'desc',\n",
    "        'title',\n",
    "        'zip_code',\n",
    "        'initial_list_status',\n",
    "        'out_prncp_inv',\n",
    "        'total_pymnt_inv',\n",
    "        'funded_amnt_inv',\n",
    "        'total_rec_prncp',\n",
    "        'total_rec_int',\n",
    "        'total_rec_late_fee',\n",
    "        'collection_recovery_fee',\n",
    "        'last_pymnt_d',\n",
    "        'last_pymnt_amnt',\n",
    "        'next_pymnt_d',\n",
    "        'last_credit_pull_d',\n",
    "        'collections_12_mths_ex_med',\n",
    "        'mths_since_last_major_derog',\n",
    "        'policy_code',\n",
    "        'application_type',\n",
    "        'annual_inc_joint',\n",
    "        'dti_joint',\n",
    "        'verification_status_joint',\n",
    "        'open_acc_6m',\n",
    "        'open_il_6m',\n",
    "        'open_il_12m',\n",
    "        'open_il_24m',\n",
    "        'mths_since_rcnt_il',\n",
    "        'total_bal_il',\n",
    "        'il_util',\n",
    "        'open_rv_12m',\n",
    "        'open_rv_24m',\n",
    "        'max_bal_bc',\n",
    "        'all_util',\n",
    "        'inq_fi',\n",
    "        'total_cu_tl',\n",
    "        'inq_last_12m',\n",
    "        'total_rev_hi_lim',\n",
    "        'open_acc',\n",
    "        'mths_since_last_record',\n",
    "        'mths_since_last_delinq'\n",
    "        ], inplace = True)\n",
    "\n",
    "df.to_csv(\"afterRemoving.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming, replacing and aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./afterRemoving.csv', index_col=0)\n",
    "\n",
    "# Remove \"months\" in column \"term\"\n",
    "df.term = df.term.str.split().str[0]\n",
    "\n",
    "# Rename column \"sub_grade\" to \"grade\"\n",
    "df.rename(columns={'sub_grade':'grade'}, inplace = True)\n",
    "\n",
    "# Normalize emp_length\n",
    "def normalize_emp_length(emp_length):\n",
    "    if (emp_length == None or (not type(emp_length) is str)):\n",
    "        return\n",
    "    if (emp_length == '< 1 year'):\n",
    "        return '0'\n",
    "    elif (emp_length == '10+ years'):\n",
    "        return '10'\n",
    "    else:\n",
    "        return emp_length.split()[0]\n",
    "\n",
    "df.emp_length = df.emp_length.apply(normalize_emp_length)\n",
    "\n",
    "df.emp_length = pd.to_numeric(df.emp_length, downcast='integer') # TODO: fix the conversion (current -> float64, desired -> int8)\n",
    "\n",
    "df.to_csv('AfterRename.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert types\n",
    "To save memory usage and processing time, the data types of the columns in the dataset were changed, accordingly to the range of the corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtc = DecisionTreeClassifier()\\n\\nfeature_cols = [\\'annual_inc\\']\\nX = df[feature_cols] # Features\\ny = df[\\'default_ind\\'].values\\n\\n(training_inputs,\\n     testing_inputs,\\n     training_classes,\\n     testing_classes) = train_test_split(X, y, test_size=0.25)\\n\\ndtc.fit(training_inputs, training_classes)\\n\\nprediction_classes = dtc.predict(testing_inputs)\\n\\n# with open(\\'credit.dot\\', \\'w\\') as out_file:\\n#    out_file = tree.export_graphviz(dtc, out_file=out_file)\\n    \\n# tree.plot_tree(dtc)\\n\\nprint(\"Confusion matrix:\\n\", metrics.confusion_matrix(testing_classes, prediction_classes))\\nprint(\"Accuracy:\",metrics.accuracy_score(testing_classes, prediction_classes))\\nprint(\"Precision:\", metrics.precision_score(testing_classes, prediction_classes, average=\\'weighted\\'))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifie\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "df = pd.read_csv('./afterRename.csv', index_col=0)\n",
    "\n",
    "def convertTypes(df):\n",
    "    df.loan_amnt = df.loan_amnt.astype('uint32')\n",
    "    df.term = df.term.astype('uint8')\n",
    "    df.int_rate = df.int_rate.astype('float16')\n",
    "    df.installment = df.installment.astype('float16')\n",
    "    df.grade = df.grade.astype('category')\n",
    "    df.home_ownership = df.home_ownership.astype('category')\n",
    "    df.annual_inc = df.annual_inc.astype('uint32')\n",
    "    df.verification_status = df.verification_status.astype('category')\n",
    "    df.purpose = df.purpose.astype('category')\n",
    "    df.addr_state = df.addr_state.astype('category')\n",
    "    df.dti = df.dti.astype('float16')\n",
    "    df.delinq_2yrs = df.delinq_2yrs.astype('uint8')\n",
    "    df.inq_last_6mths = df.inq_last_6mths.astype('uint8')\n",
    "    df.pub_rec = df.pub_rec.astype('uint8')\n",
    "    df.revol_bal = df.revol_bal.astype('uint32')\n",
    "    df.total_acc = df.total_acc.astype('uint8')\n",
    "    df.out_prncp = df.out_prncp.astype('float16')\n",
    "    df.total_pymnt = df.total_pymnt.astype('float16')\n",
    "    df.recoveries = df.recoveries.astype('float16')\n",
    "    df.acc_now_delinq = df.acc_now_delinq.astype('category')\n",
    "    df.default_ind = df.default_ind.astype('bool')   \n",
    "\n",
    "    return df\n",
    "    \n",
    "def convertTypesImputer(df):\n",
    "    df.loan_amnt = df.loan_amnt.astype('uint32')\n",
    "    df.term = df.term.astype('uint8')\n",
    "    df.int_rate = df.int_rate.astype('float16')\n",
    "    df.installment = df.installment.astype('float16')\n",
    "    df.annual_inc = df.annual_inc.astype('uint32')\n",
    "    df.dti = df.dti.astype('float16')\n",
    "    df.delinq_2yrs = df.delinq_2yrs.astype('uint8')\n",
    "    df.inq_last_6mths = df.inq_last_6mths.astype('uint8')\n",
    "    df.pub_rec = df.pub_rec.astype('uint8')\n",
    "    df.revol_bal = df.revol_bal.astype('uint32')\n",
    "    df.total_acc = df.total_acc.astype('uint8')\n",
    "    df.out_prncp = df.out_prncp.astype('float16')\n",
    "    df.total_pymnt = df.total_pymnt.astype('float16')\n",
    "    df.recoveries = df.recoveries.astype('float16')\n",
    "    df.default_ind = df.default_ind.astype('bool')\n",
    "    return df\n",
    "    \n",
    "def read_and_convert_imp(file):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df = convertTypesImputer(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_and_convert(file):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df = convertTypes(df)\n",
    "    return df\n",
    "\n",
    "df = convertTypes(df)\n",
    "df.earliest_cr_line = df.earliest_cr_line.astype('datetime64')\n",
    "df.issue_d = df.issue_d.astype('datetime64')\n",
    "\n",
    "df['since_first_cr'] = ((df['issue_d'] - df['earliest_cr_line']) / np.timedelta64(1, 'M')).astype('uint16')\n",
    "df['diff_loan_funded_amnt'] = (df['loan_amnt'] - df['funded_amnt']).astype('uint16')\n",
    "    \n",
    "df.drop(columns = [\n",
    "        'issue_d',\n",
    "        'earliest_cr_line',\n",
    "        'funded_amnt'\n",
    "        ], inplace = True)\n",
    "\n",
    "\n",
    "df.to_csv(\"afterConverting.csv\")\n",
    "#convertTypes(df)\n",
    "#df.info()\n",
    "\n",
    "# stratified_sample, _ = train_test_split(df, test_size=0.9, stratify=df[['default_ind']])\n",
    "\n",
    "\"\"\"dtc = DecisionTreeClassifier()\n",
    "\n",
    "feature_cols = ['annual_inc']\n",
    "X = df[feature_cols] # Features\n",
    "y = df['default_ind'].values\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "dtc.fit(training_inputs, training_classes)\n",
    "\n",
    "prediction_classes = dtc.predict(testing_inputs)\n",
    "\n",
    "# with open('credit.dot', 'w') as out_file:\n",
    "#    out_file = tree.export_graphviz(dtc, out_file=out_file)\n",
    "    \n",
    "# tree.plot_tree(dtc)\n",
    "\n",
    "print(\"Confusion matrix:\\n\", metrics.confusion_matrix(testing_classes, prediction_classes))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(testing_classes, prediction_classes))\n",
    "print(\"Precision:\", metrics.precision_score(testing_classes, prediction_classes, average='weighted'))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "### (Same amount of rows with default_ind = 0 and default_ind = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Put all the fraud class in a separate dataset.\\nfraud_df = shuffled_df.loc[shuffled_df['default_ind'] == 1]\\n\\n# Randomly select 46467 observations from the non-fraud (majority class)\\nnon_fraud_df = shuffled_df.loc[shuffled_df['default_ind'] == 0].sample(n=fraud_df['default_ind'].count(),random_state=42)\\n\\n# Concatenate both dataframes again\\ndf = pd.concat([fraud_df, non_fraud_df])\\n\\n# plot the dataset after the undersampling\\nplt.figure(figsize=(8, 8))\\nsb.countplot('default_ind', data=df)\\nplt.title('Balanced Classes')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_and_convert('./afterConverting.csv')\n",
    "\n",
    "# Shuffle the Dataset.\n",
    "# df = df.sample(frac=1,random_state=4)\n",
    "df, _ = train_test_split(df, test_size=0.9, stratify=df[['default_ind']]) #SAMPLE STRAT\n",
    "\n",
    "df.to_csv(\"afterSampling.csv\")\n",
    "\n",
    "\"\"\"\n",
    "# Put all the fraud class in a separate dataset.\n",
    "fraud_df = shuffled_df.loc[shuffled_df['default_ind'] == 1]\n",
    "\n",
    "# Randomly select 46467 observations from the non-fraud (majority class)\n",
    "non_fraud_df = shuffled_df.loc[shuffled_df['default_ind'] == 0].sample(n=fraud_df['default_ind'].count(),random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# plot the dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sb.countplot('default_ind', data=df)\n",
    "plt.title('Balanced Classes')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns left with missing values before imputation: \n",
      "loan_amnt                   0\n",
      "term                        0\n",
      "int_rate                    0\n",
      "installment                 0\n",
      "grade                       0\n",
      "emp_length               4191\n",
      "home_ownership              0\n",
      "annual_inc                  0\n",
      "verification_status         0\n",
      "purpose                     0\n",
      "addr_state                  0\n",
      "dti                         0\n",
      "delinq_2yrs                 0\n",
      "inq_last_6mths              0\n",
      "pub_rec                     0\n",
      "revol_bal                   0\n",
      "revol_util                 40\n",
      "total_acc                   0\n",
      "out_prncp                   0\n",
      "total_pymnt                 0\n",
      "recoveries                  0\n",
      "acc_now_delinq              0\n",
      "tot_coll_amt             6792\n",
      "tot_cur_bal              6792\n",
      "default_ind                 0\n",
      "since_first_cr              0\n",
      "diff_loan_funded_amnt       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = read_and_convert('./afterSampling.csv')\n",
    "\n",
    "print(\"Columns left with missing values before imputation: \")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "newDf = df[df.columns.difference(['grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state'])]\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "#newDf = imputer.fit_transform(df[['emp_length', 'revol_util', 'tot_coll_amt', 'tot_cur_bal', 'loan_amnt','annual_inc', 'dti', 'delinq_2yrs', 'pub_rec']])\n",
    "newDf = pd.DataFrame(imputer.fit_transform(newDf), columns = newDf.columns)\n",
    "print(\"KNNImputer completed in {} seconds\".format(time.time()-start))\n",
    "newDf.to_csv(\"Imputed.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#newDf = read_and_convert_imp(\"Imputed.csv\")\n",
    "\n",
    "# Replace emp_length | revol_util | tot_coll_amt | loan_amnt columns with calculated values\n",
    "df['emp_length'] = newDf['emp_length'].values\n",
    "df['revol_util'] = newDf['revol_util'].values\n",
    "df['tot_coll_amt'] = newDf['tot_coll_amt'].values\n",
    "df['tot_cur_bal'] = newDf['tot_cur_bal'].values\n",
    "\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(newDf[:,0] )\n",
    "\"\"\"\n",
    "df.emp_length = df.emp_length.astype('float16')\n",
    "df.revol_util = df.revol_util.astype('float16')\n",
    "df.tot_coll_amt = df.tot_coll_amt.astype('float64')\n",
    "df.tot_cur_bal = df.tot_cur_bal.astype('float64')\n",
    "\"\"\"\n",
    "\n",
    "df.to_csv(\"AfterKNNImputer.csv\")\n",
    "newDf.to_csv(\"test.csv\")\n",
    "\n",
    "# Columns with missing values\n",
    "print(\"\\nColumns left with missing values after imputation: \")\n",
    "print(df.columns[df.isnull().any()].tolist())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Outliers (Z-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv('./AfterKNNImputer.csv', index_col=0)\n",
    "\n",
    "z = np.abs(stats.zscore(df.select_dtypes(exclude=['object', 'bool'])))\n",
    "\n",
    "df.to_csv('original.csv')\n",
    "#testDf = df.select_dtypes(exclude=['object', 'bool'])\n",
    "testDf = df\n",
    "testDf = testDf[(z < 4).all(axis=1)]\n",
    "testDf.info()\n",
    "\n",
    "testDf.to_csv('test1.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_convert('./afterConverting.csv')\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_convert('./afterKNNImputer.csv')\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb.stripplot(x='grade', y='annual_inc', hue='default_ind', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.plot(subplots=True, layout=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
