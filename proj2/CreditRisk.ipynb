{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df0f38a1",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"http://www.infante.space/wp-content/uploads/2018/02/Logo-FEUP.png\" width=\"35%\"/>\n",
    "    <h1 style=\"font-size: 2.5em\">Credit Risk Analysis</h1>\n",
    "    <h2 style=\"font-size: 2em\">Artificial Intelligence 2020/21 - Supervised Learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d231a13",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [License](#License)\n",
    "\n",
    "3. [Required libraries](#Required-libraries)\n",
    "\n",
    "4. [The problem domain](#The-problem-domain)\n",
    "\n",
    "5. [Step 1: Answering the question](#Step-1:-Answering-the-question)\n",
    "\n",
    "6. [Step 2: Checking the data](#Step-2:-Checking-the-data)\n",
    "\n",
    "7. [Step 3: Tidying the data](#Step-3:-Tidying-the-data)\n",
    "\n",
    "    - [Bonus: Testing our data](#Bonus:-Testing-our-data)\n",
    "\n",
    "8. [Step 4: Exploratory analysis](#Step-4:-Exploratory-analysis)\n",
    "\n",
    "9. [Step 5: Classification](#Step-5:-Classification)\n",
    "\n",
    "    - [Cross-validation](#Cross-validation)\n",
    "\n",
    "    - [Parameter tuning](#Parameter-tuning)\n",
    "\n",
    "10. [Step 6: Reproducibility](#Step-6:-Reproducibility)\n",
    "\n",
    "11. [Conclusions](#Conclusions)\n",
    "\n",
    "12. [Further reading](#Further-reading)\n",
    "\n",
    "13. [Acknowledgements](#Acknowledgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2703a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project we look forward to use Machine Learning, more specifically, Supervised Learning, to predict the risk on loan repayment. For this, we use a provided dataset with approximatelly 800k entries of previously issued loans and aim to train this dataset using various machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081845f",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Please see the repository README file for the licenses and usage terms for the instructional material and code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d364aa",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "- **numpy** - provides support for large multidimensional arrays and matrices along with a collection of high-level mathematical functions to execute these functions swiftly.\n",
    "- **pandas** - enables the provision of easy data structure and quicker data analysis for Python. For operations like data analysis and modelling, Pandas makes it possible to carry these out without needing to switch to more domain-specific language.\n",
    "- **scikit-learn** - can be effectively used for a variety of applications which include classification, regression, clustering, model selection, naive Bayesâ€™, grade boosting, K-means, and preprocessing.\n",
    "- **matplotlib** - widely used for publication of quality figures in a variety of hard copy formats and interactive environments across platforms. Used to design charts, graphs, pie charts, scatterplots, histograms, error charts, etc.\n",
    "- **seaborn** - visualisation of statistical models like heat maps.\n",
    "- **watermark** - printing date and time stamps, version numbers, and hardware information.\n",
    "- **contextlib**\n",
    "- **time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74072740",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38ed12",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17cade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifie\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import time\n",
    "import sys\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb1e2e",
   "metadata": {},
   "source": [
    "### Cleaning and Normalization\n",
    "\n",
    "After a thorough analysis of the data available, it was decided to drop a considerable amount of columns from the dataset. It includes columns containing either data that was not relevant to the problem or columns with too many missing values to be used reliably.\n",
    "For example: identifiers, titles and descriptions; zip codes.\n",
    "We also created new columns using old ones with, for example, date differences and differences of money amounts, replaced 'grade' column with the 'sub_grade' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb580b",
   "metadata": {},
   "source": [
    "#### Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaecd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./resources/data.csv', index_col=0)\n",
    "\n",
    "# Remove id column and use a default index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.drop(columns = [\n",
    "        'member_id',\n",
    "        'grade',\n",
    "        'emp_title',\n",
    "        'pymnt_plan',\n",
    "        'desc',\n",
    "        'title',\n",
    "        'zip_code',\n",
    "        'initial_list_status',\n",
    "        'out_prncp_inv',\n",
    "        'total_pymnt_inv',\n",
    "        'funded_amnt_inv',\n",
    "        'total_rec_prncp',\n",
    "        'total_rec_int',\n",
    "        'total_rec_late_fee',\n",
    "        'collection_recovery_fee',\n",
    "        'last_pymnt_d',\n",
    "        'last_pymnt_amnt',\n",
    "        'next_pymnt_d',\n",
    "        'last_credit_pull_d',\n",
    "        'collections_12_mths_ex_med',\n",
    "        'mths_since_last_major_derog',\n",
    "        'policy_code',\n",
    "        'application_type',\n",
    "        'annual_inc_joint',\n",
    "        'dti_joint',\n",
    "        'verification_status_joint',\n",
    "        'open_acc_6m',\n",
    "        'open_il_6m',\n",
    "        'open_il_12m',\n",
    "        'open_il_24m',\n",
    "        'mths_since_rcnt_il',\n",
    "        'total_bal_il',\n",
    "        'il_util',\n",
    "        'open_rv_12m',\n",
    "        'open_rv_24m',\n",
    "        'max_bal_bc',\n",
    "        'all_util',\n",
    "        'inq_fi',\n",
    "        'total_cu_tl',\n",
    "        'inq_last_12m',\n",
    "        'total_rev_hi_lim',\n",
    "        'open_acc',\n",
    "        'mths_since_last_record',\n",
    "        'mths_since_last_delinq'\n",
    "        ], inplace = True)\n",
    "\n",
    "df.to_csv(\"afterRemoving.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4d40a",
   "metadata": {},
   "source": [
    "#### Renaming, replacing and aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041fe76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./afterRemoving.csv', index_col=0)\n",
    "\n",
    "# Remove \"months\" in column \"term\"\n",
    "df.term = df.term.str.split().str[0]\n",
    "\n",
    "# Rename column \"sub_grade\" to \"grade\"\n",
    "df.rename(columns={'sub_grade':'grade'}, inplace = True)\n",
    "\n",
    "# Normalize emp_length\n",
    "def normalize_emp_length(emp_length):\n",
    "    if (emp_length == None or (not type(emp_length) is str)):\n",
    "        return\n",
    "    if (emp_length == '< 1 year'):\n",
    "        return '0'\n",
    "    elif (emp_length == '10+ years'):\n",
    "        return '10'\n",
    "    else:\n",
    "        return emp_length.split()[0]\n",
    "\n",
    "df.emp_length = df.emp_length.apply(normalize_emp_length)\n",
    "\n",
    "df.emp_length = pd.to_numeric(df.emp_length, downcast='integer') # TODO: fix the conversion (current -> float64, desired -> int8)\n",
    "\n",
    "df.to_csv('afterRename.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3490ae",
   "metadata": {},
   "source": [
    "### Convert types\n",
    "To save memory usage and processing time, the data types of the columns in the dataset were changed, accordingly to the range of the corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b258de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifie\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "df = pd.read_csv('./afterRename.csv', index_col=0)\n",
    "\n",
    "def convertTypes(df):\n",
    "    df.loan_amnt = df.loan_amnt.astype('uint32')\n",
    "    df.term = df.term.astype('uint8')\n",
    "    df.int_rate = df.int_rate.astype('float16')\n",
    "    df.installment = df.installment.astype('float16')\n",
    "    df.grade = df.grade.astype('category')\n",
    "    df.home_ownership = df.home_ownership.astype('category')\n",
    "    df.annual_inc = df.annual_inc.astype('uint32')\n",
    "    df.verification_status = df.verification_status.astype('category')\n",
    "    df.purpose = df.purpose.astype('category')\n",
    "    df.addr_state = df.addr_state.astype('category')\n",
    "    df.dti = df.dti.astype('float16')\n",
    "    df.delinq_2yrs = df.delinq_2yrs.astype('uint8')\n",
    "    df.inq_last_6mths = df.inq_last_6mths.astype('uint8')\n",
    "    df.pub_rec = df.pub_rec.astype('uint8')\n",
    "    df.revol_bal = df.revol_bal.astype('uint32')\n",
    "    df.total_acc = df.total_acc.astype('uint8')\n",
    "    df.out_prncp = df.out_prncp.astype('float16')\n",
    "    df.total_pymnt = df.total_pymnt.astype('float16')\n",
    "    df.recoveries = df.recoveries.astype('float16')\n",
    "    df.acc_now_delinq = df.acc_now_delinq.astype('category')\n",
    "    df.default_ind = df.default_ind.astype('bool')   \n",
    "\n",
    "    return df\n",
    "    \n",
    "def convertTypesImputer(df):\n",
    "    df.loan_amnt = df.loan_amnt.astype('uint32')\n",
    "    df.term = df.term.astype('uint8')\n",
    "    df.int_rate = df.int_rate.astype('float16')\n",
    "    df.installment = df.installment.astype('float16')\n",
    "    df.annual_inc = df.annual_inc.astype('uint32')\n",
    "    df.dti = df.dti.astype('float16')\n",
    "    df.delinq_2yrs = df.delinq_2yrs.astype('uint8')\n",
    "    df.inq_last_6mths = df.inq_last_6mths.astype('uint8')\n",
    "    df.pub_rec = df.pub_rec.astype('uint8')\n",
    "    df.revol_bal = df.revol_bal.astype('uint32')\n",
    "    df.total_acc = df.total_acc.astype('uint8')\n",
    "    df.out_prncp = df.out_prncp.astype('float16')\n",
    "    df.total_pymnt = df.total_pymnt.astype('float16')\n",
    "    df.recoveries = df.recoveries.astype('float16')\n",
    "    df.default_ind = df.default_ind.astype('bool')\n",
    "    return df\n",
    "    \n",
    "def read_and_convert_imp(file):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df = convertTypesImputer(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_and_convert(file):\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    df = convertTypes(df)\n",
    "    return df\n",
    "\n",
    "df = convertTypes(df)\n",
    "df.earliest_cr_line = df.earliest_cr_line.astype('datetime64')\n",
    "df.issue_d = df.issue_d.astype('datetime64')\n",
    "\n",
    "df['since_first_cr'] = ((df['issue_d'] - df['earliest_cr_line']) / np.timedelta64(1, 'M')).astype('uint16')\n",
    "df['diff_loan_funded_amnt'] = (df['loan_amnt'] - df['funded_amnt']).astype('uint16')\n",
    "    \n",
    "df.drop(columns = [\n",
    "        'issue_d',\n",
    "        'earliest_cr_line',\n",
    "        'funded_amnt'\n",
    "        ], inplace = True)\n",
    "\n",
    "\n",
    "df.to_csv(\"afterConverting.csv\")\n",
    "#convertTypes(df)\n",
    "#df.info()\n",
    "\n",
    "# stratified_sample, _ = train_test_split(df, test_size=0.9, stratify=df[['default_ind']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced84362",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "### (Same amount of rows with default_ind = 0 and default_ind = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058477e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_convert('./afterConverting.csv')\n",
    "\n",
    "# plot the dataset before the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sb.countplot('default_ind', data=df)\n",
    "plt.title('Unbalanced Classes before sampling')\n",
    "plt.show()\n",
    "\n",
    "# Shuffle the Dataset.\n",
    "df_train, df_test = train_test_split(df, test_size=0.9, stratify=df[['default_ind']]) #SAMPLE STRAT\n",
    "\n",
    "df_train.to_csv(\"afterSampling.csv\")\n",
    "\n",
    "\n",
    "# Put all the fraud class in a separate dataset.\n",
    "fraud_df = df_train.loc[df_train['default_ind'] == 1]\n",
    "\n",
    "# Randomly select 46467 observations from the non-fraud (majority class)\n",
    "non_fraud_df = df_train.loc[df['default_ind'] == 0].sample(n=fraud_df['default_ind'].count(),random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "df_train = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# plot the train dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sb.countplot('default_ind', data=df_train)\n",
    "plt.title('Train dataset')\n",
    "plt.show()\n",
    "\n",
    "# plot the test dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sb.countplot('default_ind', data=df_test)\n",
    "plt.title('Test dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebc665",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_convert('./afterSampling.csv')\n",
    "\n",
    "print(\"Columns left with missing values before imputation: \")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "newDf = df[df.columns.difference(['grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state'])]\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "#newDf = imputer.fit_transform(df[['emp_length', 'revol_util', 'tot_coll_amt', 'tot_cur_bal', 'loan_amnt','annual_inc', 'dti', 'delinq_2yrs', 'pub_rec']])\n",
    "newDf = pd.DataFrame(imputer.fit_transform(newDf), columns = newDf.columns)\n",
    "print(\"KNNImputer completed in {} seconds\".format(time.time()-start))\n",
    "newDf.to_csv(\"Imputed.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#newDf = read_and_convert_imp(\"Imputed.csv\")\n",
    "\n",
    "# Replace emp_length | revol_util | tot_coll_amt | loan_amnt columns with calculated values\n",
    "df['emp_length'] = newDf['emp_length'].values\n",
    "df['revol_util'] = newDf['revol_util'].values\n",
    "df['tot_coll_amt'] = newDf['tot_coll_amt'].values\n",
    "df['tot_cur_bal'] = newDf['tot_cur_bal'].values\n",
    "\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(newDf[:,0] )\n",
    "\"\"\"\n",
    "df.emp_length = df.emp_length.astype('float16')\n",
    "df.revol_util = df.revol_util.astype('float16')\n",
    "df.tot_coll_amt = df.tot_coll_amt.astype('float64')\n",
    "df.tot_cur_bal = df.tot_cur_bal.astype('float64')\n",
    "\"\"\"\n",
    "\n",
    "df.to_csv(\"AfterKNNImputer.csv\")\n",
    "\n",
    "df.to_csv(\"data_clean.csv\")\n",
    "\n",
    "# Columns with missing values\n",
    "print(\"\\nColumns left with missing values after imputation: \")\n",
    "print(df.columns[df.isnull().any()].tolist())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af85a27",
   "metadata": {},
   "source": [
    "## Removal of Outliers (Z-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02777170-7d8c-4658-98a0-327eccfe59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def remove_outliers(_df):\n",
    "    z = np.abs(stats.zscore(_df.select_dtypes(exclude=['object', 'bool', 'category'])))\n",
    "\n",
    "    #_df.to_csv('original.csv')\n",
    "    #testDf = df.select_dtypes(exclude=['object', 'bool'])\n",
    "    testDf = _df\n",
    "    testDf = testDf[(z < 4).all(axis=1)]\n",
    "    #testDf.info()\n",
    "\n",
    "    #testDf.to_csv('test1.csv')\n",
    "    return testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d5876",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8c829",
   "metadata": {},
   "source": [
    "### Before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd247c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_convert('./afterConverting.csv')\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b787a4",
   "metadata": {},
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08285250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_and_convert('./AfterKNNImputer.csv')\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36730a9b-584d-4a5e-89b6-59a4b9d4f67f",
   "metadata": {},
   "source": [
    "# Plots\n",
    "## Relation between the loan amount, the annual income and the home ownership \n",
    "\n",
    "The circular points represent the accepted loans, and the cross points represent the declined ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb642bc8-24b4-40fa-8274-75c592498bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_outliers = df\n",
    "df_without_outliers = remove_outliers(df_without_outliers)\n",
    "\n",
    "scatter = sb.relplot(x=\"loan_amnt\", y=\"annual_inc\", hue=\"home_ownership\", style=\"default_ind\", data=df_without_outliers.sample(n=2000,random_state=42), aspect=2)\n",
    "scatter.savefig(\"plot1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00634f-5044-4ba2-8184-991b4a6a58ac",
   "metadata": {},
   "source": [
    "### Pair plot\n",
    "#### Before the removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a149cad-8e65-4c61-a68f-66a65e73fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_plot(df,output_name):\n",
    "    columns_to_use = [\"loan_amnt\",\"int_rate\",\"installment\",\"emp_length\",\"annual_inc\",\"dti\",\"delinq_2yrs\",\"inq_last_6mths\",\"pub_rec\",\"revol_bal\",\"revol_util\",\"total_acc\",\"out_prncp\",\"total_pymnt\",\"tot_coll_amt\",\"tot_cur_bal\",\"since_first_cr\"]\n",
    "\n",
    "    columns_not_to_use = [\"grade\",\"home_ownership\",\"verification_status\",\"purpose\",\"addr_state\",\"acc_now_delinq\",\"term\",\"default_ind\",\"diff_loan_funded_amnt\",\"recoveries\"] # just to know what columns are not being used in the graph\n",
    "\n",
    "    pair_plot = sb.pairplot(df.sample(n=2000,random_state=42), hue='default_ind', vars=columns_to_use, corner=True, height=4)\n",
    "\n",
    "    # don't render the plot in the browser, to improve performance\n",
    "    pair_plot.savefig(output_name)\n",
    "    #plt.clf()\n",
    "\n",
    "pair_plot(df, \"pair_plot_before.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208b2bf-3d5c-4ac6-9358-4c5cc6b05c12",
   "metadata": {},
   "source": [
    "#### After the removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce40f2-2ded-4593-8334-d0ffecad1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_plot(df_without_outliers, \"pair_plot_after.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd0d3d0",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8a371",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8008ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_clean.csv\", index_col=0)\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "dtc_df = df\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "columnsToEncode = list(dtc_df.select_dtypes(include=['category','object', 'bool']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        dtc_df[feature] = le.fit_transform(dtc_df[feature])\n",
    "    except:\n",
    "        print('Error encoding '+feature)\n",
    "\n",
    "X = dtc_df[df.columns.difference(['default_ind'])].values # Features\n",
    "y = dtc_df['default_ind'].values\n",
    "\n",
    "dtc_df.to_csv(\"testtt.csv\")\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(X, y, test_size=0.25)\n",
    "                \n",
    "dtc.fit(training_inputs, training_classes)\n",
    "print(len(training_inputs))\n",
    "print(len(testing_inputs))\n",
    "prediction_classes = dtc.predict(testing_inputs)\n",
    "\n",
    "# with open('credit.dot', 'w') as out_file:\n",
    "#    out_file = tree.export_graphviz(dtc, out_file=out_file)\n",
    "    \n",
    "# tree.plot_tree(dtc)\n",
    "\n",
    "print(\"Confusion matrix:\\n\", metrics.confusion_matrix(testing_classes, prediction_classes))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(testing_classes, prediction_classes))\n",
    "print(\"Precision:\", metrics.precision_score(testing_classes, prediction_classes, average='weighted'))\n",
    "\n",
    "\n",
    "\"\"\" ax = plt.gca()\n",
    "svc_disp = metrics.plot_roc_curve(svc, testing_inputs, testing_classes, ax=ax, alpha=0.8) \"\"\"\n",
    "#dtc_disp.plot(ax=ax, alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(10):\n",
    "    (training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(X, y, test_size=0.90)\n",
    "                \n",
    "    dtc.fit(training_inputs, training_classes)\n",
    "    classifier_accuracy = dtc.score(testing_inputs, testing_classes)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "plt.hist(model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373ac5b-c9a5-44ff-962d-5e083b752803",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b50c11-232c-4727-b2a7-66403eeeccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from numpy import std\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def plot_cv(cv, features, labels):\n",
    "    masks = []\n",
    "    for train, test in cv.split(features, labels):\n",
    "        mask = np.zeros(len(labels), dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "    \n",
    "    plt.figure(figsize=(10, 200))\n",
    "    plt.imshow(masks, interpolation='none', cmap='gray_r')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.xlabel('Row #')\n",
    "\n",
    "all_inputs = X\n",
    "all_labels = y\n",
    "# plot_cv(StratifiedKFold(n_splits=10), all_inputs[:100], all_labels[:100])\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, scoring='accuracy', cv=10)\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)) + ' (%.3f)' % std(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82f15f-a0c8-47c9-8338-7ef1817cb22b",
   "metadata": {},
   "source": [
    "# Repeated Cross-Validation\n",
    "\n",
    "### Box Plot\n",
    "The orange line indicates the median of the distribution and the green triangle represents the arithmetic mean. If these symbols (values) coincide, it suggests a reasonable symmetric distribution and that the mean may capture the central tendency well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0f05f-c6d1-4317-9e38-628a88a924b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "repeats = range(1,6)\n",
    "results = list()\n",
    "\n",
    "for r in repeats:\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=r, random_state=1)\n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "    # cross_val_score returns a list of the scores, which we can visualize\n",
    "    # to get a reasonable estimate of our classifier's performance\n",
    "    cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, scoring='accuracy', cv=cv)\n",
    "    \n",
    "    results.append(cv_scores)\n",
    "\n",
    "plt.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b20bb-d07a-4c7e-938b-b82a99d72581",
   "metadata": {},
   "source": [
    "# Stratified Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f333205-b4a5-48f0-8077-d355a22c8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "target = y\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "def train_model(train, test, fold_no):\n",
    "    X_train = train[train.columns.difference(['default_ind'])]\n",
    "    y_train = train['default_ind']\n",
    "    X_test = test[train.columns.difference(['default_ind'])]\n",
    "    y_test = test['default_ind']\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Fold', str(fold_no), 'Accuracy:', accuracy_score(y_test, predictions))\n",
    "\n",
    "\n",
    "fold_no = 1\n",
    "for train_index, test_index in skf.split(dtc_df, target):\n",
    "    train = dtc_df.iloc[train_index]\n",
    "    test = dtc_df.iloc[test_index]\n",
    "    #print('Fold', str(fold_no), 'Class Ratio:', sum(test['default_ind']) / len(test['default_ind']))\n",
    "    train_model(train, test, fold_no) \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07decfbe-3e69-44c9-a861-bad9deb1aa35",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**COPIED:**\n",
    "\n",
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbed6f-5e2d-4c1f-883f-73eab2e3b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(training_inputs, training_classes)\n",
    "y_pred = logreg.predict(testing_inputs)\n",
    "\n",
    "\n",
    "confusion_matrix = pd.crosstab(testing_classes, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sb.heatmap(confusion_matrix, annot=True)\n",
    "print('Accuracy of logistic regression (liblinear) classifier on test set: {:.2f}'.format(logreg.score(testing_inputs, testing_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1768a-277e-4bd6-ae99-ceec6b9b3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "X_train = mm_scaler.fit_transform(X_train)\n",
    "X_test = mm_scaler.transform(X_test)\n",
    "X_test = mm_scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_score = logreg.score(X_test, y_test)\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sb.heatmap(confusion_matrix, annot=True)\n",
    "print('Accuracy of logistic regression (lbfgs) classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b79ed-f78f-4473-b706-42f193c11aa3",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf4b2b-4f6d-4dda-8d55-cdab6071b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"data_clean.csv\", index_col=0)\n",
    "svc_df = df\n",
    "\n",
    "columnsToEncode = list(svc_df.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        svc_df[feature] = le.fit_transform(svc_df[feature])\n",
    "    except:\n",
    "        print('Error encoding '+feature)\n",
    "        \n",
    "X = svc_df[df.columns.difference(['default_ind'])].values # Features\n",
    "y = svc_df['default_ind'].values\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(X, y, test_size=0.25, stratify=df[['default_ind']])\n",
    "\n",
    "svc = SVC(kernel='linear', class_weight='balanced')\n",
    "scaler = StandardScaler()\n",
    "training_inputs = scaler.fit_transform(training_inputs)\n",
    "start = time.time()\n",
    "print(\"SVM fit...\")\n",
    "svc.fit(training_inputs, training_classes)\n",
    "print(\"SVM fit in {} seconds\".format(time.time()-start))    \n",
    "print(\"SVM predicting...\")\n",
    "start = time.time()\n",
    "pred = svc.predict(testing_inputs)\n",
    "print(\"SVM completed in {} seconds\".format(time.time()-start))\n",
    "\n",
    "print(metrics.confusion_matrix(testing_classes, pred))\n",
    "print(metrics.classification_report(testing_classes, pred))"
   ]
  },
  {
   "source": [
    "# Deep Learning with Neural Network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"data_clean.csv\", index_col=0)\n",
    "neural_network_df = df\n",
    "\n",
    "columnsToEncode = list(neural_network_df.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        neural_network_df[feature] = le.fit_transform(neural_network_df[feature])\n",
    "    except:\n",
    "        print('Error encoding '+feature)\n",
    "        \n",
    "X = svc_df[df.columns.difference(['default_ind'])].values # Features\n",
    "y = neural_network_df['default_ind'].values\n",
    "\n",
    "(training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(X, y, test_size=0.25, stratify=df[['default_ind']], random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(training_inputs)\n",
    "training_inputs = scaler.transform(training_inputs)\n",
    "testing_inputs = scaler.transform(testing_inputs)\n",
    "\n",
    "\n",
    "neural_network_model = Sequential()\n",
    "\n",
    "neural_network_model.add(Dense(8, activation='relu', input_shape=(26,)))\n",
    "neural_network_model.add(Dense(8, activation='relu'))\n",
    "neural_network_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "neural_network_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "history = neural_network_model.fit(training_inputs, training_classes,epochs=8, batch_size=1, verbose=1)\n",
    "print(\"Neural Network fit in {} seconds\".format(time.time()-start))\n",
    "pred = neural_network_model.predict(testing_inputs)\n",
    "\n",
    "score = neural_network_model.evaluate(testing_inputs, testing_classes,verbose=1)\n",
    "\n",
    "print(\"Score:\", score)\n",
    "print(\"\\nSummary:\")\n",
    "neural_network_model.summary()\n",
    "\n",
    "print(\"\\nPlotting...\")\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(neural_network_model, to_file='neural_network.png', show_shapes=True,)\n",
    "\n",
    "# save model in file to run in netron\n",
    "neural_network_model.save('netron_model.h5')\n"
   ]
  },
  {
   "source": [
    "### Model Visualization using [Netron](https://github.com/lutzroeder/netron):\n",
    "\n",
    "\n",
    "![Model Visualization](netron.png \"Model Visualization\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa560c-54bf-4531-9291-ffe7d9e34a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "models = [\n",
    "    (\"Decision Tree\", dtc),\n",
    "    (\"Support Vector Machine\", svc),\n",
    "    (\"Logistic Regretion\", logreg),\n",
    "    #(\"Neural Network\", neural_network_model) \n",
    "]\n",
    "\n",
    "model_displays = {}\n",
    "for name, pipeline in models:\n",
    "    model_displays[name] = metrics.plot_roc_curve(\n",
    "        pipeline, testing_inputs, testing_classes, ax=ax, name=name)\n",
    "_ = ax.set_title('ROC curve')\n",
    "\n",
    "\n",
    "plt.savefig(\"roc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}